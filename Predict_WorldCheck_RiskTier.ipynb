{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of Notebook\n",
    "\n",
    "This notebook predicts Risk Rating of each record in the World-check data and generates a csv file with Risk Rating assigned to each tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace 'sample_data.csv' with actual data when running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utsav\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (8,9,10,14,22,24,26,29,30,31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('sample_data.csv', sep='\\t', encoding='windows-1252')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform all the pre-processing steps such as lemmatizing, lower case conversion, removing stop words & punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(data):\n",
    "     \"\"\"lemmatizes content of each world-check record\"\"\"\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + WordNetLemmatizer().lemmatize(w)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    "    \"\"\"converts content of each world-check record into lower case\"\"\"\n",
    "    return np.char.lower(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "     \"\"\"removes stop words from each world-check record\"\"\"\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    \"\"\"removes punctuations from each world-check record\"\"\"\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(data):\n",
    "    \"\"\"removes apostrophe from the each world-check record\"\"\"\n",
    "    return np.char.replace(data, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    \"\"\"single function that combines all the preprocessing functions\n",
    "       You may notice that some functions are called more than once that is \n",
    "       beccause after lemmatizing step you may encounter few stop words again \n",
    "    \"\"\"\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) \n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = lemmatize_stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = lemmatize_stemming(data) \n",
    "    data = remove_punctuation(data) \n",
    "    data = remove_stop_words(data) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new column that cleans 'Further Information' column of world-check which we will use to predict the Risk rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_furtherinfo'] = df['FURTHER INFORMATION'].apply(preprocess) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load necessary Pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('high_list.pickle', 'rb') as handle:\n",
    "    high = pickle.load(handle)\n",
    "\n",
    "high_crime = []\n",
    "\n",
    "for h in high:\n",
    "    high_crime.append(h[0])\n",
    "\n",
    "high_crime.append('victim')\n",
    "high_crime.append('labor')\n",
    "\n",
    "\n",
    "high_crime.remove('report')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('med_list.pickle', 'rb') as handle:\n",
    "    med = pickle.load(handle)\n",
    "\n",
    "med_crime = []\n",
    "\n",
    "for m in med:\n",
    "    med_crime.append(m[0]) \n",
    "\n",
    "med_crime.append('license')\n",
    "med_crime.remove('report')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('low_list.pickle', 'rb') as handle:\n",
    "    low = pickle.load(handle)\n",
    "\n",
    "low_crime = []\n",
    "\n",
    "for l in low:\n",
    "    low_crime.append(l[0])   \n",
    "    \n",
    "low_crime.append('trespass')\n",
    "low_crime.remove('report')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('high_embed.pickle', 'rb') as handle:\n",
    "    high_embed = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('med_embed.pickle', 'rb') as handle:\n",
    "    med_embed = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('low_embed.pickle', 'rb') as handle:\n",
    "    low_embed = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf_dict.pickle', 'rb') as handle:\n",
    "    dictionary = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained word2vec model from Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    \"\"\"Tokenize the further information column of world-check\"\"\"\n",
    "    tokens = word_tokenize(data)\n",
    "    return (list(set(tokens)))\n",
    "\n",
    "df['tokens'] = df.processed_furtherinfo.apply(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the tokenized words with words attached to each tier to determine the match of tier\n",
    "\n",
    "This method of predicting Risk Tier didn't yield accepted result, so we decided not to consider this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highcheck(data):\n",
    "    new_text = \"\"\n",
    "    for d in data:\n",
    "        if d in high_crime:\n",
    "            new_text = new_text + \" \" + d\n",
    "    return new_text\n",
    "\n",
    "df['high_severe'] = df.tokens.apply(highcheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medcheck(data):\n",
    "    new_text = \"\"\n",
    "    for d in data:\n",
    "        if d in med_crime:\n",
    "            new_text = new_text + \" \" + d\n",
    "    return new_text\n",
    "\n",
    "df['med_severe'] = df.tokens.apply(medcheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowcheck(data):\n",
    "    new_text = \"\"\n",
    "    for d in data:\n",
    "        if d in low_crime:\n",
    "            new_text = new_text + \" \" + d\n",
    "    return new_text\n",
    "\n",
    "df['low_severe'] = df.tokens.apply(lowcheck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the score of each matched word from Tf-idf vector and add the scores to get the final score\n",
    "\n",
    "This method was comparatively more accurate than previous method but it still didn't yield the expected results, so we decided not to consider this method either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_weight(data):\n",
    "    x = tokenize(data)\n",
    "    z = 0\n",
    "    for x1 in x:\n",
    "        z = z + (next(score for (crime, score) in high if crime == x1))\n",
    "    return z\n",
    "\n",
    "df['high_score'] = df.high_severe.apply(high_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def med_weight(data):\n",
    "    x = tokenize(data)\n",
    "    z = 0\n",
    "    for x1 in x:\n",
    "        z = z + (next(score for (crime, score) in med if crime == x1))\n",
    "    return z\n",
    "\n",
    "df['med_score'] = df.med_severe.apply(med_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_weight(data):\n",
    "    x = tokenize(data)\n",
    "    z = 0\n",
    "    for x1 in x:\n",
    "        z = z + (next(score for (crime, score) in low if crime == x1))\n",
    "    return z\n",
    "\n",
    "df['low_score'] = df.low_severe.apply(low_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the word embedding of each record and compare it with the generated word embedding of each risk tier using Cosine distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the tf-idf weighted word embedding by averaging the word embedding for each record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(data): \n",
    "    wordlist = tokenize(data)\n",
    "    wordvecs = np.zeros((len(wordlist),300))\n",
    "    for i,w in enumerate(wordlist):\n",
    "        try:\n",
    "            weight = dictionary[w]\n",
    "            wordvecs[i,:] = weight*model.get_vector(w.lower())\n",
    "        except Exception as e:\n",
    "            wordvecs[i,:] = np.zeros((1,300))\n",
    "            #print(e)\n",
    "            \n",
    "    sentence = np.mean(wordvecs,0)\n",
    "    #return sentence\n",
    "    if np.sum(sentence)!=0:\n",
    "        return sentence\n",
    "    else:\n",
    "        return np.NaN  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to generate cosine distance of each tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utsav\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def high_sim(data):\n",
    "    embed1 = get_embed(data)\n",
    "    cos_sim = np.dot(embed1, high_embed) / (np.linalg.norm(embed1) * np.linalg.norm(high_embed))\n",
    "    if np.isnan(np.sum(cos_sim)):\n",
    "        return 0\n",
    "    return cos_sim\n",
    "\n",
    "df['high_similarity'] = df.high_severe.apply(high_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def med_sim(data):\n",
    "    embed1 = get_embed(data)\n",
    "    cos_sim = np.dot(embed1, med_embed) / (np.linalg.norm(embed1) * np.linalg.norm(med_embed))\n",
    "    if np.isnan(np.sum(cos_sim)):\n",
    "        return 0\n",
    "    return cos_sim\n",
    "\n",
    "df['med_similarity'] = df.med_severe.apply(med_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_sim(data):\n",
    "    embed1 = get_embed(data)\n",
    "    cos_sim = np.dot(embed1, low_embed) / (np.linalg.norm(embed1) * np.linalg.norm(low_embed))\n",
    "    if np.isnan(np.sum(cos_sim)):\n",
    "        return 0\n",
    "    return cos_sim\n",
    "\n",
    "df['low_similarity'] = df.low_severe.apply(low_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Decide Risk tier by assigning Risk Rating of the highest score for each record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['based_on_score']=df[['high_score', 'med_score', 'low_score']].idxmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['based_on_similarity']=df[['high_similarity', 'med_similarity', 'low_similarity']].idxmax(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the data accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_name(data):\n",
    "    if data == 'high_score':\n",
    "        data = 'high'\n",
    "    elif data == 'med_score':\n",
    "        data = 'medium'\n",
    "    elif data == 'low_score':\n",
    "        data = 'low'\n",
    "    elif data == 'high_similarity':\n",
    "        data = 'high'\n",
    "    elif data == 'med_similarity':\n",
    "        data = 'medium'\n",
    "    elif data == 'low_similarity':\n",
    "        data = 'low'\n",
    "    return data\n",
    "\n",
    "df['rating_based_on_weight']=df.based_on_score.apply(change_name)\n",
    "df['rating_based_on_similarity']=df.based_on_similarity.apply(change_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the risk calculated world-check data into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('riskcaluclated_worldcheck_new.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df['rating_based_on_similarity'] == 'high']\n",
    "df1.to_csv('high_rated_worldcheck.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df['rating_based_on_similarity'] == 'medium']\n",
    "df2.to_csv('medium_rated_worldcheck.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df[df['rating_based_on_similarity'] == 'low']\n",
    "df3.to_csv('low_rated_worldcheck.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
